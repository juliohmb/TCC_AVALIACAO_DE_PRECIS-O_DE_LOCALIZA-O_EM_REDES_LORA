{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97df40da-d7cc-4670-8dec-e4f2a2ce6e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 11:05:38.008986: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.13/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas importadas. TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import joblib\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "# Verifica a versão do TensorFlow\n",
    "print(f\"Bibliotecas importadas. TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a2e6de9-81a0-4851-95a9-60444cc2129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurações carregadas de 'config.json'.\n",
      "Configurações de treinamento definidas.\n"
     ]
    }
   ],
   "source": [
    "# Configurações de Visualização e Pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Criação de diretórios de saída\n",
    "OUTPUT_IMG_DIR = \"imagens\"\n",
    "os.makedirs(OUTPUT_IMG_DIR, exist_ok=True)\n",
    "\n",
    "# Caminhos Padrão\n",
    "DATASET_PATH = 'lorawan_antwerp_2019_dataset.json'\n",
    "GATEWAYS_PATH = 'lorawan_antwerp_gateway_locations.json'\n",
    "\n",
    "# Carregamento de Configurações (config.json)\n",
    "try:\n",
    "    with open('config.json', 'r') as f:\n",
    "        CONFIG = json.load(f)\n",
    "    print(\"Configurações carregadas de 'config.json'.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"AVISO: 'config.json' não encontrado. Usando configurações padrão.\")\n",
    "    CONFIG = {}\n",
    "\n",
    "# Configurações Específicas da Rede Neural (Fallback robusto)\n",
    "NN_CONFIG = CONFIG.get('nn_config', {\n",
    "    \"gt_interval_seconds\": 600, # 10 min\n",
    "    \"hidden_neurons\": 10, \n",
    "    \"epochs\": 4000,\n",
    "    \"batch_size\": 32, \n",
    "    \"test_split_size\": 0.2, \n",
    "    \"validation_split_size\": 0.1,\n",
    "    \"patience_early_stop\": 50, \n",
    "    \"default_rssi\": -120.0,\n",
    "    \"model_save_path\": \"modelo_nn_lora_gt.h5\",\n",
    "    \"input_scaler_save_path\": \"input_scaler.joblib\",\n",
    "    \"output_scaler_save_path\": \"output_scaler.joblib\"\n",
    "})\n",
    "\n",
    "print(\"Configurações de treinamento definidas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "892a9b2a-fb8e-4c07-be37-54072de4c289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Calcula a distância em metros entre duas coordenadas (Fórmula de Haversine).\"\"\"\n",
    "    R = 6371000  # Raio da Terra em metros\n",
    "    if any(v is None for v in [lat1, lon1, lat2, lon2]): \n",
    "        return float('inf')\n",
    "    \n",
    "    phi1, phi2 = map(math.radians, [lat1, lat2])\n",
    "    dphi = math.radians(lat2 - lat1)\n",
    "    dlambda = math.radians(lon2 - lon1)\n",
    "    \n",
    "    a = math.sin(dphi / 2)**2 + math.cos(phi1) * math.cos(phi2) * math.sin(dlambda / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def latlon_to_xy(lat, lon, ref_lat, ref_lon):\n",
    "    \"\"\"Converte Lat/Lon para coordenadas cartesianas locais (X, Y) em metros.\"\"\"\n",
    "    if any(v is None for v in [lat, lon]): \n",
    "        return None, None\n",
    "        \n",
    "    x = haversine(ref_lat, ref_lon, ref_lat, lon) * np.sign(lon - ref_lon)\n",
    "    y = haversine(ref_lat, ref_lon, lat, ref_lon) * np.sign(lat - ref_lat)\n",
    "    return x, y\n",
    "\n",
    "def extract_rssi_ordered(gateways_data, gw_locations_ordered_ids, default_rssi):\n",
    "    \"\"\"\n",
    "    Extrai RSSIs garantindo a ordem das colunas para a Rede Neural.\n",
    "    Preenche gateways ausentes com 'default_rssi'.\n",
    "    \"\"\"\n",
    "    if not isinstance(gateways_data, list):\n",
    "        return [default_rssi] * len(gw_locations_ordered_ids)\n",
    "\n",
    "    rssi_dict = {\n",
    "        gw['id']: gw['rssi'] \n",
    "        for gw in gateways_data \n",
    "        if 'rssi' in gw and gw['id'] in gw_locations_ordered_ids\n",
    "    }\n",
    "    \n",
    "    return [rssi_dict.get(gw_id, default_rssi) for gw_id in gw_locations_ordered_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db5cd830-f793-4183-b17f-f79121207d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo matriz de RSSI...\n",
      "Dados base carregados: 129830 amostras.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>dev_eui</th>\n",
       "      <th>gt_x</th>\n",
       "      <th>gt_y</th>\n",
       "      <th>rssi_004A026B</th>\n",
       "      <th>rssi_08060346</th>\n",
       "      <th>rssi_08060388</th>\n",
       "      <th>rssi_080605EE</th>\n",
       "      <th>rssi_08060622</th>\n",
       "      <th>rssi_08060716</th>\n",
       "      <th>rssi_080E0098</th>\n",
       "      <th>rssi_080E00B9</th>\n",
       "      <th>rssi_080E0116</th>\n",
       "      <th>rssi_080E0149</th>\n",
       "      <th>rssi_FF0102CD</th>\n",
       "      <th>rssi_FF01031C</th>\n",
       "      <th>rssi_FF01052A</th>\n",
       "      <th>rssi_FF01053C</th>\n",
       "      <th>rssi_FF010548</th>\n",
       "      <th>rssi_FF01054A</th>\n",
       "      <th>rssi_FF010560</th>\n",
       "      <th>rssi_FF010562</th>\n",
       "      <th>rssi_FF010565</th>\n",
       "      <th>rssi_FF010566</th>\n",
       "      <th>rssi_FF01056A</th>\n",
       "      <th>rssi_FF01056B</th>\n",
       "      <th>rssi_FF01056D</th>\n",
       "      <th>rssi_FF01056F</th>\n",
       "      <th>rssi_FF010575</th>\n",
       "      <th>rssi_FF010580</th>\n",
       "      <th>rssi_FF010581</th>\n",
       "      <th>rssi_FF010582</th>\n",
       "      <th>rssi_FF010584</th>\n",
       "      <th>rssi_FF01058C</th>\n",
       "      <th>rssi_FF01058D</th>\n",
       "      <th>rssi_FF01058F</th>\n",
       "      <th>rssi_FF010591</th>\n",
       "      <th>rssi_FF010594</th>\n",
       "      <th>rssi_FF010595</th>\n",
       "      <th>rssi_FF010596</th>\n",
       "      <th>rssi_FF010597</th>\n",
       "      <th>rssi_FF010598</th>\n",
       "      <th>rssi_FF01059B</th>\n",
       "      <th>rssi_FF0105A0</th>\n",
       "      <th>rssi_FF0105AC</th>\n",
       "      <th>rssi_FF0105B3</th>\n",
       "      <th>rssi_FF0105B5</th>\n",
       "      <th>rssi_FF0105B8</th>\n",
       "      <th>rssi_FF0105BB</th>\n",
       "      <th>rssi_FF0105BE</th>\n",
       "      <th>rssi_FF0105C2</th>\n",
       "      <th>rssi_FF0105C4</th>\n",
       "      <th>rssi_FF0105CB</th>\n",
       "      <th>rssi_FF0105CF</th>\n",
       "      <th>rssi_FF0105D6</th>\n",
       "      <th>rssi_FF0105DB</th>\n",
       "      <th>rssi_FF0105E5</th>\n",
       "      <th>rssi_FF0105E6</th>\n",
       "      <th>rssi_FF0105EC</th>\n",
       "      <th>rssi_FF0105F5</th>\n",
       "      <th>rssi_FF0105F6</th>\n",
       "      <th>rssi_FF0105F9</th>\n",
       "      <th>rssi_FF0105FA</th>\n",
       "      <th>rssi_FF0105FE</th>\n",
       "      <th>rssi_FF010600</th>\n",
       "      <th>rssi_FF010601</th>\n",
       "      <th>rssi_FF010602</th>\n",
       "      <th>rssi_FF010603</th>\n",
       "      <th>rssi_FF010604</th>\n",
       "      <th>rssi_FF010605</th>\n",
       "      <th>rssi_FF010606</th>\n",
       "      <th>rssi_FF010607</th>\n",
       "      <th>rssi_FF01060B</th>\n",
       "      <th>rssi_FF01061C</th>\n",
       "      <th>rssi_FF01061E</th>\n",
       "      <th>rssi_FF010622</th>\n",
       "      <th>rssi_FF010628</th>\n",
       "      <th>rssi_FF01062B</th>\n",
       "      <th>rssi_FF01062F</th>\n",
       "      <th>rssi_FF010639</th>\n",
       "      <th>rssi_FF010653</th>\n",
       "      <th>rssi_FF010654</th>\n",
       "      <th>rssi_FF010662</th>\n",
       "      <th>rssi_FF010666</th>\n",
       "      <th>rssi_FF010667</th>\n",
       "      <th>rssi_FF010668</th>\n",
       "      <th>rssi_FF01066B</th>\n",
       "      <th>rssi_FF010673</th>\n",
       "      <th>rssi_FF010675</th>\n",
       "      <th>rssi_FF010684</th>\n",
       "      <th>rssi_FF010714</th>\n",
       "      <th>rssi_FF010715</th>\n",
       "      <th>rssi_FF010718</th>\n",
       "      <th>rssi_FF010719</th>\n",
       "      <th>rssi_FF010720</th>\n",
       "      <th>rssi_FF010721</th>\n",
       "      <th>rssi_FF010722</th>\n",
       "      <th>rssi_FF01072B</th>\n",
       "      <th>rssi_FF01072D</th>\n",
       "      <th>rssi_FF010733</th>\n",
       "      <th>rssi_FF010739</th>\n",
       "      <th>rssi_FF01073A</th>\n",
       "      <th>rssi_FF01076A</th>\n",
       "      <th>rssi_FF010771</th>\n",
       "      <th>rssi_FF010784</th>\n",
       "      <th>rssi_FF010789</th>\n",
       "      <th>rssi_FF010791</th>\n",
       "      <th>rssi_FF010795</th>\n",
       "      <th>rssi_FF010797</th>\n",
       "      <th>rssi_FF010798</th>\n",
       "      <th>rssi_FF0107A1</th>\n",
       "      <th>rssi_FF0107A2</th>\n",
       "      <th>rssi_FF0107A3</th>\n",
       "      <th>rssi_FF0107A5</th>\n",
       "      <th>rssi_FF0107A6</th>\n",
       "      <th>rssi_FF0107A7</th>\n",
       "      <th>rssi_FF0107A8</th>\n",
       "      <th>rssi_FF0107B3</th>\n",
       "      <th>rssi_FF0107BB</th>\n",
       "      <th>rssi_FF0107BC</th>\n",
       "      <th>rssi_FF0107BF</th>\n",
       "      <th>rssi_FF0107C2</th>\n",
       "      <th>rssi_FF0107C7</th>\n",
       "      <th>rssi_FF0107C9</th>\n",
       "      <th>rssi_FF0107E3</th>\n",
       "      <th>rssi_FF0107E9</th>\n",
       "      <th>rssi_FF0107EB</th>\n",
       "      <th>rssi_FF0107EC</th>\n",
       "      <th>rssi_FF0107F1</th>\n",
       "      <th>rssi_FF0107F2</th>\n",
       "      <th>rssi_FF0107F5</th>\n",
       "      <th>rssi_FF0107F8</th>\n",
       "      <th>rssi_FF0107FE</th>\n",
       "      <th>rssi_FF0107FF</th>\n",
       "      <th>rssi_FF010802</th>\n",
       "      <th>rssi_FF010806</th>\n",
       "      <th>rssi_FF010807</th>\n",
       "      <th>rssi_FF010809</th>\n",
       "      <th>rssi_FF010817</th>\n",
       "      <th>rssi_FF01081C</th>\n",
       "      <th>rssi_FF01081D</th>\n",
       "      <th>rssi_FF010822</th>\n",
       "      <th>rssi_FF010824</th>\n",
       "      <th>rssi_FF01082C</th>\n",
       "      <th>rssi_FF010840</th>\n",
       "      <th>rssi_FF010858</th>\n",
       "      <th>rssi_FF01085B</th>\n",
       "      <th>rssi_FF01085C</th>\n",
       "      <th>rssi_FF01085E</th>\n",
       "      <th>rssi_FF010868</th>\n",
       "      <th>rssi_FF010869</th>\n",
       "      <th>rssi_FF010877</th>\n",
       "      <th>rssi_FF010879</th>\n",
       "      <th>rssi_FF01087A</th>\n",
       "      <th>rssi_FF01087D</th>\n",
       "      <th>rssi_FF010884</th>\n",
       "      <th>rssi_FF010885</th>\n",
       "      <th>rssi_FF010889</th>\n",
       "      <th>rssi_FF01088C</th>\n",
       "      <th>rssi_FF010891</th>\n",
       "      <th>rssi_FF010894</th>\n",
       "      <th>rssi_FF010896</th>\n",
       "      <th>rssi_FF01089B</th>\n",
       "      <th>rssi_FF01089C</th>\n",
       "      <th>rssi_FF01089E</th>\n",
       "      <th>rssi_FF0108A3</th>\n",
       "      <th>rssi_FF0108A5</th>\n",
       "      <th>rssi_FF0108E0</th>\n",
       "      <th>rssi_FF0108ED</th>\n",
       "      <th>rssi_FF010930</th>\n",
       "      <th>rssi_FF01093F</th>\n",
       "      <th>rssi_FF010947</th>\n",
       "      <th>rssi_FF010957</th>\n",
       "      <th>rssi_FF01095C</th>\n",
       "      <th>rssi_FF01095D</th>\n",
       "      <th>rssi_FF010965</th>\n",
       "      <th>rssi_FF010974</th>\n",
       "      <th>rssi_FF010979</th>\n",
       "      <th>rssi_FF01097A</th>\n",
       "      <th>rssi_FF010984</th>\n",
       "      <th>rssi_FF010987</th>\n",
       "      <th>rssi_FF010988</th>\n",
       "      <th>rssi_FF01098C</th>\n",
       "      <th>rssi_FF01098F</th>\n",
       "      <th>rssi_FF010991</th>\n",
       "      <th>rssi_FF010998</th>\n",
       "      <th>rssi_FF010999</th>\n",
       "      <th>rssi_FF01099B</th>\n",
       "      <th>rssi_FF0109B7</th>\n",
       "      <th>rssi_FF0109DA</th>\n",
       "      <th>rssi_FF010A99</th>\n",
       "      <th>rssi_FF010B26</th>\n",
       "      <th>rssi_FF010B36</th>\n",
       "      <th>rssi_FF010B51</th>\n",
       "      <th>rssi_FF010B62</th>\n",
       "      <th>rssi_FF010B7D</th>\n",
       "      <th>rssi_FF010B9B</th>\n",
       "      <th>rssi_FF010BF7</th>\n",
       "      <th>rssi_FF010C42</th>\n",
       "      <th>rssi_FF010C4A</th>\n",
       "      <th>rssi_FF010C7C</th>\n",
       "      <th>rssi_FF010C86</th>\n",
       "      <th>rssi_FF010C8E</th>\n",
       "      <th>rssi_FF010CB4</th>\n",
       "      <th>rssi_FF010E11</th>\n",
       "      <th>rssi_FF010E66</th>\n",
       "      <th>rssi_FF010E7C</th>\n",
       "      <th>rssi_FF010E83</th>\n",
       "      <th>rssi_FF010E89</th>\n",
       "      <th>rssi_FF010E8F</th>\n",
       "      <th>rssi_FF010E91</th>\n",
       "      <th>rssi_FF010EA8</th>\n",
       "      <th>rssi_FF010EA9</th>\n",
       "      <th>rssi_FF010EAC</th>\n",
       "      <th>rssi_FF010ED2</th>\n",
       "      <th>rssi_FF010ED4</th>\n",
       "      <th>rssi_FF010EE1</th>\n",
       "      <th>rssi_FF010EFB</th>\n",
       "      <th>rssi_FF010EFE</th>\n",
       "      <th>rssi_FF010EFF</th>\n",
       "      <th>rssi_FF010F35</th>\n",
       "      <th>rssi_FF010F41</th>\n",
       "      <th>rssi_FF01753E</th>\n",
       "      <th>rssi_FF01753F</th>\n",
       "      <th>rssi_FF01763F</th>\n",
       "      <th>rssi_FF017641</th>\n",
       "      <th>rssi_FF017642</th>\n",
       "      <th>rssi_FF017643</th>\n",
       "      <th>rssi_FF017645</th>\n",
       "      <th>rssi_FF017648</th>\n",
       "      <th>rssi_FF017649</th>\n",
       "      <th>rssi_FF017663</th>\n",
       "      <th>rssi_FF017666</th>\n",
       "      <th>rssi_FF01773B</th>\n",
       "      <th>rssi_FF01775F</th>\n",
       "      <th>rssi_FF017773</th>\n",
       "      <th>rssi_FF0177DB</th>\n",
       "      <th>rssi_FF01783D</th>\n",
       "      <th>rssi_FF0178DF</th>\n",
       "      <th>rssi_FF0178F5</th>\n",
       "      <th>rssi_FF0178F9</th>\n",
       "      <th>rssi_FF0178FB</th>\n",
       "      <th>rssi_FF017927</th>\n",
       "      <th>rssi_FF017929</th>\n",
       "      <th>rssi_FF017946</th>\n",
       "      <th>rssi_FF017959</th>\n",
       "      <th>rssi_FF01795A</th>\n",
       "      <th>rssi_FF017967</th>\n",
       "      <th>rssi_FF0179CF</th>\n",
       "      <th>rssi_FF017A31</th>\n",
       "      <th>rssi_FF017A8E</th>\n",
       "      <th>rssi_FF017AA8</th>\n",
       "      <th>rssi_FF017AB5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-11-30 11:08:25.907540156+01:00</td>\n",
       "      <td>343233384B376D18</td>\n",
       "      <td>-1844.3864</td>\n",
       "      <td>-1129.5780</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-117.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-109.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-106.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-11-30 12:08:27.402852065+01:00</td>\n",
       "      <td>343233384B376D18</td>\n",
       "      <td>-1834.1614</td>\n",
       "      <td>-1117.7011</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-114.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-113.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-11-16 13:01:25.387000+01:00</td>\n",
       "      <td>343233384D378718</td>\n",
       "      <td>-275.1458</td>\n",
       "      <td>-3752.6761</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-116.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "      <td>-120.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            timestamp           dev_eui       gt_x       gt_y  \\\n",
       "0 2018-11-30 11:08:25.907540156+01:00  343233384B376D18 -1844.3864 -1129.5780   \n",
       "1 2018-11-30 12:08:27.402852065+01:00  343233384B376D18 -1834.1614 -1117.7011   \n",
       "2    2018-11-16 13:01:25.387000+01:00  343233384D378718  -275.1458 -3752.6761   \n",
       "\n",
       "   rssi_004A026B  rssi_08060346  rssi_08060388  rssi_080605EE  rssi_08060622  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_08060716  rssi_080E0098  rssi_080E00B9  rssi_080E0116  rssi_080E0149  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -117.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF0102CD  rssi_FF01031C  rssi_FF01052A  rssi_FF01053C  rssi_FF010548  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF01054A  rssi_FF010560  rssi_FF010562  rssi_FF010565  rssi_FF010566  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF01056A  rssi_FF01056B  rssi_FF01056D  rssi_FF01056F  rssi_FF010575  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010580  rssi_FF010581  rssi_FF010582  rssi_FF010584  rssi_FF01058C  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF01058D  rssi_FF01058F  rssi_FF010591  rssi_FF010594  rssi_FF010595  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -109.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -114.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010596  rssi_FF010597  rssi_FF010598  rssi_FF01059B  rssi_FF0105A0  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF0105AC  rssi_FF0105B3  rssi_FF0105B5  rssi_FF0105B8  rssi_FF0105BB  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF0105BE  rssi_FF0105C2  rssi_FF0105C4  rssi_FF0105CB  rssi_FF0105CF  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF0105D6  rssi_FF0105DB  rssi_FF0105E5  rssi_FF0105E6  rssi_FF0105EC  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF0105F5  rssi_FF0105F6  rssi_FF0105F9  rssi_FF0105FA  rssi_FF0105FE  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010600  rssi_FF010601  rssi_FF010602  rssi_FF010603  rssi_FF010604  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010605  rssi_FF010606  rssi_FF010607  rssi_FF01060B  rssi_FF01061C  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF01061E  rssi_FF010622  rssi_FF010628  rssi_FF01062B  rssi_FF01062F  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010639  rssi_FF010653  rssi_FF010654  rssi_FF010662  rssi_FF010666  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010667  rssi_FF010668  rssi_FF01066B  rssi_FF010673  rssi_FF010675  \\\n",
       "0      -106.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -113.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010684  rssi_FF010714  rssi_FF010715  rssi_FF010718  rssi_FF010719  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -116.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010720  rssi_FF010721  rssi_FF010722  rssi_FF01072B  rssi_FF01072D  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010733  rssi_FF010739  rssi_FF01073A  rssi_FF01076A  rssi_FF010771  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010784  rssi_FF010789  rssi_FF010791  rssi_FF010795  rssi_FF010797  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010798  rssi_FF0107A1  rssi_FF0107A2  rssi_FF0107A3  rssi_FF0107A5  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF0107A6  rssi_FF0107A7  rssi_FF0107A8  rssi_FF0107B3  rssi_FF0107BB  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF0107BC  rssi_FF0107BF  rssi_FF0107C2  rssi_FF0107C7  rssi_FF0107C9  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF0107E3  rssi_FF0107E9  rssi_FF0107EB  rssi_FF0107EC  rssi_FF0107F1  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF0107F2  rssi_FF0107F5  rssi_FF0107F8  rssi_FF0107FE  rssi_FF0107FF  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010802  rssi_FF010806  rssi_FF010807  rssi_FF010809  rssi_FF010817  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF01081C  rssi_FF01081D  rssi_FF010822  rssi_FF010824  rssi_FF01082C  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010840  rssi_FF010858  rssi_FF01085B  rssi_FF01085C  rssi_FF01085E  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010868  rssi_FF010869  rssi_FF010877  rssi_FF010879  rssi_FF01087A  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF01087D  rssi_FF010884  rssi_FF010885  rssi_FF010889  rssi_FF01088C  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010891  rssi_FF010894  rssi_FF010896  rssi_FF01089B  rssi_FF01089C  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF01089E  rssi_FF0108A3  rssi_FF0108A5  rssi_FF0108E0  rssi_FF0108ED  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010930  rssi_FF01093F  rssi_FF010947  rssi_FF010957  rssi_FF01095C  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF01095D  rssi_FF010965  rssi_FF010974  rssi_FF010979  rssi_FF01097A  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010984  rssi_FF010987  rssi_FF010988  rssi_FF01098C  rssi_FF01098F  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010991  rssi_FF010998  rssi_FF010999  rssi_FF01099B  rssi_FF0109B7  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF0109DA  rssi_FF010A99  rssi_FF010B26  rssi_FF010B36  rssi_FF010B51  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010B62  rssi_FF010B7D  rssi_FF010B9B  rssi_FF010BF7  rssi_FF010C42  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010C4A  rssi_FF010C7C  rssi_FF010C86  rssi_FF010C8E  rssi_FF010CB4  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010E11  rssi_FF010E66  rssi_FF010E7C  rssi_FF010E83  rssi_FF010E89  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010E8F  rssi_FF010E91  rssi_FF010EA8  rssi_FF010EA9  rssi_FF010EAC  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010ED2  rssi_FF010ED4  rssi_FF010EE1  rssi_FF010EFB  rssi_FF010EFE  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF010EFF  rssi_FF010F35  rssi_FF010F41  rssi_FF01753E  rssi_FF01753F  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF01763F  rssi_FF017641  rssi_FF017642  rssi_FF017643  rssi_FF017645  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF017648  rssi_FF017649  rssi_FF017663  rssi_FF017666  rssi_FF01773B  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF01775F  rssi_FF017773  rssi_FF0177DB  rssi_FF01783D  rssi_FF0178DF  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF0178F5  rssi_FF0178F9  rssi_FF0178FB  rssi_FF017927  rssi_FF017929  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF017946  rssi_FF017959  rssi_FF01795A  rssi_FF017967  rssi_FF0179CF  \\\n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000      -120.0000   \n",
       "\n",
       "   rssi_FF017A31  rssi_FF017A8E  rssi_FF017AA8  rssi_FF017AB5  \n",
       "0      -120.0000      -120.0000      -120.0000      -120.0000  \n",
       "1      -120.0000      -120.0000      -120.0000      -120.0000  \n",
       "2      -120.0000      -120.0000      -120.0000      -120.0000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_and_process_base_data():\n",
    "    \"\"\"Carrega JSONs, filtra HDOP e gera matriz de features base (RSSI + XY).\"\"\"\n",
    "    try:\n",
    "        with open(GATEWAYS_PATH, 'r') as f:\n",
    "            gateway_locations = json.load(f)\n",
    "        df = pd.read_json(DATASET_PATH)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Erro fatal: Arquivo {e.filename} não encontrado.\")\n",
    "        return pd.DataFrame(), []\n",
    "\n",
    "    # 1. Definição de Ordem dos Gateways\n",
    "    gateway_ids_ordered = sorted(gateway_locations.keys())\n",
    "    \n",
    "    # 2. Filtragem e Limpeza\n",
    "    hdop_limit = CONFIG.get('hdop_threshold', 5.0)\n",
    "    df = df[df['hdop'] <= hdop_limit].copy()\n",
    "    \n",
    "    # Extração segura de timestamp\n",
    "    df['timestamp'] = pd.to_datetime([\n",
    "        g[0]['rx_time']['time'] if g and isinstance(g, list) and 'rx_time' in g[0] else None \n",
    "        for g in df['gateways']\n",
    "    ], errors='coerce')\n",
    "    \n",
    "    df.dropna(subset=['timestamp', 'dev_eui', 'latitude', 'longitude', 'gateways'], inplace=True)\n",
    "    \n",
    "    if df.empty: return df, gateway_ids_ordered\n",
    "\n",
    "    # 3. Conversão Alvo (GT) para XY Local\n",
    "    global_ref_lat = df.iloc[0]['latitude']\n",
    "    global_ref_lon = df.iloc[0]['longitude']\n",
    "    \n",
    "    gt_coords = df.apply(\n",
    "        lambda row: latlon_to_xy(row['latitude'], row['longitude'], global_ref_lat, global_ref_lon), \n",
    "        axis=1\n",
    "    )\n",
    "    df[['gt_x', 'gt_y']] = pd.DataFrame(gt_coords.tolist(), index=df.index)\n",
    "\n",
    "    # 4. Extração de Features RSSI (Vetorizada)\n",
    "    print(\"Extraindo matriz de RSSI...\")\n",
    "    rssi_matrix = df['gateways'].apply(\n",
    "        lambda g: extract_rssi_ordered(g, gateway_ids_ordered, NN_CONFIG['default_rssi'])\n",
    "    )\n",
    "    \n",
    "    df_rssi = pd.DataFrame(\n",
    "        rssi_matrix.tolist(), \n",
    "        columns=[f'rssi_{gid}' for gid in gateway_ids_ordered], \n",
    "        index=df.index\n",
    "    )\n",
    "\n",
    "    # 5. Consolidação\n",
    "    df_final = pd.concat([df[['timestamp', 'dev_eui', 'gt_x', 'gt_y']], df_rssi], axis=1)\n",
    "    df_final.dropna(subset=['gt_x', 'gt_y'], inplace=True)\n",
    "    df_final.sort_values(by=['dev_eui', 'timestamp'], inplace=True)\n",
    "    \n",
    "    return df_final.reset_index(drop=True), gateway_ids_ordered\n",
    "\n",
    "df_features_base, GATEWAY_IDS_ORDERED = load_and_process_base_data()\n",
    "if not df_features_base.empty:\n",
    "    print(f\"Dados base carregados: {len(df_features_base)} amostras.\")\n",
    "    display(df_features_base.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52bf8ec8-33a1-4f6f-b901-9ea745595471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aplicando features temporais (Last GT)...\n",
      "Dataset pronto para treino: 129830 amostras.\n"
     ]
    }
   ],
   "source": [
    "def create_temporal_features(group):\n",
    "    \"\"\"\n",
    "    Gera features baseadas no último Ground Truth conhecido (Last GT).\n",
    "    Lógica: Atualiza o 'last_gt' a cada X segundos definidos na config.\n",
    "    \"\"\"\n",
    "    group = group.copy()\n",
    "    gt_interval = NN_CONFIG['gt_interval_seconds']\n",
    "    \n",
    "    last_x, last_y, last_time = np.nan, np.nan, pd.NaT\n",
    "    \n",
    "    last_gt_x_list, last_gt_y_list, last_gt_time_list = [], [], []\n",
    "\n",
    "    # Iteração necessária devido à dependência sequencial temporal\n",
    "    for ts, cur_x, cur_y in zip(group['timestamp'], group['gt_x'], group['gt_y']):\n",
    "        if pd.isna(last_time) or (ts - last_time).total_seconds() >= gt_interval:\n",
    "            last_x, last_y, last_time = cur_x, cur_y, ts\n",
    "            \n",
    "        last_gt_x_list.append(last_x)\n",
    "        last_gt_y_list.append(last_y)\n",
    "        last_gt_time_list.append(last_time)\n",
    "\n",
    "    group['last_gt_x'] = last_gt_x_list\n",
    "    group['last_gt_y'] = last_gt_y_list\n",
    "    \n",
    "    # Cálculo vetorizado do Delta T\n",
    "    last_times_series = pd.to_datetime(last_gt_time_list, utc=True)\n",
    "    group['delta_t'] = (group['timestamp'] - last_times_series).dt.total_seconds()\n",
    "    \n",
    "    return group\n",
    "\n",
    "print(\"Aplicando features temporais (Last GT)...\")\n",
    "if not df_features_base.empty:\n",
    "    df_features_final = df_features_base.groupby('dev_eui', group_keys=False).apply(create_temporal_features)\n",
    "    \n",
    "    # Limpeza final\n",
    "    df_features_final.dropna(subset=['last_gt_x', 'last_gt_y', 'delta_t'], inplace=True)\n",
    "    df_features_final = df_features_final[df_features_final['delta_t'] >= 0]\n",
    "    \n",
    "    print(f\"Dataset pronto para treino: {len(df_features_final)} amostras.\")\n",
    "else:\n",
    "    df_features_final = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a151e13-c23b-4a03-a5d2-91969843dfdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando dados (StandardScaler)...\n",
      "Shapes -> Treino: (90881, 252), Val: (12983, 252), Teste: (25966, 252)\n"
     ]
    }
   ],
   "source": [
    "if not df_features_final.empty:\n",
    "    # Definição de Colunas\n",
    "    feature_cols = [c for c in df_features_final.columns if c.startswith('rssi_')] + \\\n",
    "                   ['last_gt_x', 'last_gt_y', 'delta_t']\n",
    "    target_cols = ['gt_x', 'gt_y']\n",
    "    \n",
    "    # Split Treino/Teste\n",
    "    train_val, test_df = train_test_split(\n",
    "        df_features_final, test_size=NN_CONFIG['test_split_size'], random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Split Treino/Validação\n",
    "    val_ratio_adj = NN_CONFIG['validation_split_size'] / (1.0 - NN_CONFIG['test_split_size'])\n",
    "    train_df, val_df = train_test_split(\n",
    "        train_val, test_size=val_ratio_adj, random_state=42, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Scaling\n",
    "    print(\"Normalizando dados (StandardScaler)...\")\n",
    "    input_scaler = StandardScaler()\n",
    "    output_scaler = StandardScaler()\n",
    "    \n",
    "    # Fit apenas no treino\n",
    "    X_train = input_scaler.fit_transform(train_df[feature_cols])\n",
    "    y_train = output_scaler.fit_transform(train_df[target_cols])\n",
    "    \n",
    "    # Transform nos outros\n",
    "    X_val = input_scaler.transform(val_df[feature_cols])\n",
    "    y_val = output_scaler.transform(val_df[target_cols])\n",
    "    \n",
    "    X_test = input_scaler.transform(test_df[feature_cols])\n",
    "    y_test = output_scaler.transform(test_df[target_cols])\n",
    "    \n",
    "    # Persistência dos Scalers\n",
    "    joblib.dump(input_scaler, NN_CONFIG['input_scaler_save_path'])\n",
    "    joblib.dump(output_scaler, NN_CONFIG['output_scaler_save_path'])\n",
    "    \n",
    "    print(f\"Shapes -> Treino: {X_train.shape}, Val: {X_val.shape}, Teste: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b9870a7-4e79-467e-a8eb-5444986a120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1763733961.575261   10015 gpu_device.cc:2020] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3444 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"NN_LoRa_Locator\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"NN_LoRa_Locator\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Hidden_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,530</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Regressao (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Hidden_Layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,530\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Regressao (\u001b[38;5;33mDense\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m22\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,552</span> (9.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,552\u001b[0m (9.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,552</span> (9.97 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,552\u001b[0m (9.97 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_model(input_dim, output_dim, hidden_units):\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_dim,), name=\"Input\"),\n",
    "        layers.Dense(hidden_units, activation=\"relu\", name=\"Hidden_Layer\"),\n",
    "        layers.Dense(output_dim, activation=\"linear\", name=\"Output_Regressao\")\n",
    "    ], name=\"NN_LoRa_Locator\")\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        loss='mae',\n",
    "        metrics=['mse', 'mae']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "if 'X_train' in locals():\n",
    "    model = build_model(X_train.shape[1], y_train.shape[1], NN_CONFIG['hidden_neurons'])\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce081ec7-0446-413c-b272-acb8d7c67979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando treinamento...\n",
      "Epoch 1/4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-21 11:06:03.419256: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 91608048 exceeds 10% of free system memory.\n",
      "2025-11-21 11:06:03.484166: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 91608048 exceeds 10% of free system memory.\n",
      "2025-11-21 11:06:04.164453: I external/local_xla/xla/service/service.cc:163] XLA service 0x73f25c00a8d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-11-21 11:06:04.164467: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce GTX 1050, Compute Capability 6.1\n",
      "2025-11-21 11:06:04.220830: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-11-21 11:06:04.412247: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  61/2841\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - loss: 0.6183 - mae: 0.6183 - mse: 0.6920"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1763733965.091405   10144 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1990 - mae: 0.1990 - mse: 0.1386"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - loss: 0.1329 - mae: 0.1329 - mse: 0.0697 - val_loss: 0.0992 - val_mae: 0.0992 - val_mse: 0.0431\n",
      "Epoch 2/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.1007 - mae: 0.1007 - mse: 0.0440"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0994 - mae: 0.0994 - mse: 0.0437 - val_loss: 0.0952 - val_mae: 0.0952 - val_mse: 0.0417\n",
      "Epoch 3/4000\n",
      "\u001b[1m2832/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0974 - mae: 0.0974 - mse: 0.0431"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0965 - mae: 0.0965 - mse: 0.0428 - val_loss: 0.0940 - val_mae: 0.0940 - val_mse: 0.0404\n",
      "Epoch 4/4000\n",
      "\u001b[1m2819/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0955 - mae: 0.0955 - mse: 0.0430"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0947 - mae: 0.0947 - mse: 0.0420 - val_loss: 0.0912 - val_mae: 0.0912 - val_mse: 0.0400\n",
      "Epoch 5/4000\n",
      "\u001b[1m2826/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0932 - mae: 0.0932 - mse: 0.0415"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0934 - mae: 0.0934 - mse: 0.0414 - val_loss: 0.0907 - val_mae: 0.0907 - val_mse: 0.0389\n",
      "Epoch 6/4000\n",
      "\u001b[1m2833/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0927 - mae: 0.0927 - mse: 0.0402"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0926 - mae: 0.0926 - mse: 0.0406 - val_loss: 0.0889 - val_mae: 0.0889 - val_mse: 0.0389\n",
      "Epoch 7/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0919 - mae: 0.0919 - mse: 0.0400 - val_loss: 0.0898 - val_mae: 0.0898 - val_mse: 0.0376\n",
      "Epoch 8/4000\n",
      "\u001b[1m2815/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0917 - mae: 0.0917 - mse: 0.0393"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0912 - mae: 0.0912 - mse: 0.0392 - val_loss: 0.0889 - val_mae: 0.0889 - val_mse: 0.0370\n",
      "Epoch 9/4000\n",
      "\u001b[1m2823/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0911 - mae: 0.0911 - mse: 0.0380"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0907 - mae: 0.0907 - mse: 0.0383 - val_loss: 0.0879 - val_mae: 0.0879 - val_mse: 0.0362\n",
      "Epoch 10/4000\n",
      "\u001b[1m2838/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0907 - mae: 0.0907 - mse: 0.0389"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0901 - mae: 0.0901 - mse: 0.0378 - val_loss: 0.0868 - val_mae: 0.0868 - val_mse: 0.0359\n",
      "Epoch 11/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0896 - mae: 0.0896 - mse: 0.0375 - val_loss: 0.0878 - val_mae: 0.0878 - val_mse: 0.0358\n",
      "Epoch 12/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0895 - mae: 0.0895 - mse: 0.0373 - val_loss: 0.0873 - val_mae: 0.0873 - val_mse: 0.0352\n",
      "Epoch 13/4000\n",
      "\u001b[1m2826/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0888 - mae: 0.0888 - mse: 0.0358"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0891 - mae: 0.0891 - mse: 0.0370 - val_loss: 0.0864 - val_mae: 0.0864 - val_mse: 0.0354\n",
      "Epoch 14/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0890 - mae: 0.0890 - mse: 0.0370 - val_loss: 0.0867 - val_mae: 0.0867 - val_mse: 0.0354\n",
      "Epoch 15/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0889 - mae: 0.0889 - mse: 0.0369 - val_loss: 0.0867 - val_mae: 0.0867 - val_mse: 0.0357\n",
      "Epoch 16/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0887 - mae: 0.0887 - mse: 0.0368 - val_loss: 0.0866 - val_mae: 0.0866 - val_mse: 0.0355\n",
      "Epoch 17/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0886 - mae: 0.0886 - mse: 0.0367 - val_loss: 0.0876 - val_mae: 0.0876 - val_mse: 0.0349\n",
      "Epoch 18/4000\n",
      "\u001b[1m2816/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0885 - mae: 0.0885 - mse: 0.0359"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0886 - mae: 0.0886 - mse: 0.0368 - val_loss: 0.0859 - val_mae: 0.0859 - val_mse: 0.0346\n",
      "Epoch 19/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0885 - mae: 0.0885 - mse: 0.0367 - val_loss: 0.0861 - val_mae: 0.0861 - val_mse: 0.0347\n",
      "Epoch 20/4000\n",
      "\u001b[1m2824/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0885 - mae: 0.0885 - mse: 0.0364"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0884 - mae: 0.0884 - mse: 0.0367 - val_loss: 0.0859 - val_mae: 0.0859 - val_mse: 0.0350\n",
      "Epoch 21/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0884 - mae: 0.0884 - mse: 0.0366 - val_loss: 0.0863 - val_mae: 0.0863 - val_mse: 0.0353\n",
      "Epoch 22/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0882 - mae: 0.0882 - mse: 0.0366 - val_loss: 0.0864 - val_mae: 0.0864 - val_mse: 0.0349\n",
      "Epoch 23/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0881 - mae: 0.0881 - mse: 0.0366 - val_loss: 0.0859 - val_mae: 0.0859 - val_mse: 0.0350\n",
      "Epoch 24/4000\n",
      "\u001b[1m2811/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0885 - mae: 0.0885 - mse: 0.0367"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0881 - mae: 0.0881 - mse: 0.0365 - val_loss: 0.0857 - val_mae: 0.0857 - val_mse: 0.0347\n",
      "Epoch 25/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0880 - mae: 0.0880 - mse: 0.0365 - val_loss: 0.0860 - val_mae: 0.0860 - val_mse: 0.0353\n",
      "Epoch 26/4000\n",
      "\u001b[1m2840/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0869 - mae: 0.0869 - mse: 0.0346"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0879 - mae: 0.0879 - mse: 0.0365 - val_loss: 0.0849 - val_mae: 0.0849 - val_mse: 0.0342\n",
      "Epoch 27/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0877 - mae: 0.0877 - mse: 0.0364 - val_loss: 0.0858 - val_mae: 0.0858 - val_mse: 0.0345\n",
      "Epoch 28/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0876 - mae: 0.0876 - mse: 0.0363 - val_loss: 0.0853 - val_mae: 0.0853 - val_mse: 0.0344\n",
      "Epoch 29/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0877 - mae: 0.0877 - mse: 0.0364 - val_loss: 0.0855 - val_mae: 0.0855 - val_mse: 0.0348\n",
      "Epoch 30/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0876 - mae: 0.0876 - mse: 0.0363 - val_loss: 0.0850 - val_mae: 0.0850 - val_mse: 0.0349\n",
      "Epoch 31/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0875 - mae: 0.0875 - mse: 0.0362 - val_loss: 0.0854 - val_mae: 0.0854 - val_mse: 0.0352\n",
      "Epoch 32/4000\n",
      "\u001b[1m2839/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0875 - mae: 0.0875 - mse: 0.0368"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0874 - mae: 0.0874 - mse: 0.0362 - val_loss: 0.0842 - val_mae: 0.0842 - val_mse: 0.0343\n",
      "Epoch 33/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0874 - mae: 0.0874 - mse: 0.0361 - val_loss: 0.0864 - val_mae: 0.0864 - val_mse: 0.0349\n",
      "Epoch 34/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0874 - mae: 0.0874 - mse: 0.0362 - val_loss: 0.0854 - val_mae: 0.0854 - val_mse: 0.0344\n",
      "Epoch 35/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0873 - mae: 0.0873 - mse: 0.0362 - val_loss: 0.0850 - val_mae: 0.0850 - val_mse: 0.0342\n",
      "Epoch 36/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0872 - mae: 0.0872 - mse: 0.0361 - val_loss: 0.0843 - val_mae: 0.0843 - val_mse: 0.0345\n",
      "Epoch 37/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0871 - mae: 0.0871 - mse: 0.0361 - val_loss: 0.0858 - val_mae: 0.0858 - val_mse: 0.0344\n",
      "Epoch 38/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0871 - mae: 0.0871 - mse: 0.0360 - val_loss: 0.0854 - val_mae: 0.0854 - val_mse: 0.0339\n",
      "Epoch 39/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0871 - mae: 0.0871 - mse: 0.0359 - val_loss: 0.0845 - val_mae: 0.0845 - val_mse: 0.0343\n",
      "Epoch 40/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0869 - mae: 0.0869 - mse: 0.0359 - val_loss: 0.0844 - val_mae: 0.0844 - val_mse: 0.0342\n",
      "Epoch 41/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0868 - mae: 0.0868 - mse: 0.0358 - val_loss: 0.0847 - val_mae: 0.0847 - val_mse: 0.0341\n",
      "Epoch 42/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0866 - mae: 0.0866 - mse: 0.0357 - val_loss: 0.0846 - val_mae: 0.0846 - val_mse: 0.0339\n",
      "Epoch 43/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0865 - mae: 0.0865 - mse: 0.0356 - val_loss: 0.0844 - val_mae: 0.0844 - val_mse: 0.0341\n",
      "Epoch 44/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0864 - mae: 0.0864 - mse: 0.0355 - val_loss: 0.0845 - val_mae: 0.0845 - val_mse: 0.0341\n",
      "Epoch 45/4000\n",
      "\u001b[1m2840/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.0865 - mae: 0.0865 - mse: 0.0355"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0863 - mae: 0.0863 - mse: 0.0356 - val_loss: 0.0835 - val_mae: 0.0835 - val_mse: 0.0342\n",
      "Epoch 46/4000\n",
      "\u001b[1m2841/2841\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - loss: 0.0863 - mae: 0.0863 - mse: 0.0355 - val_loss: 0.0845 - val_mae: 0.0845 - val_mse: 0.0339\n",
      "Epoch 47/4000\n",
      "\u001b[1m 331/2841\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.0880 - mae: 0.0880 - mse: 0.0360"
     ]
    }
   ],
   "source": [
    "if 'model' in locals() and model is not None:\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=NN_CONFIG['patience_early_stop'], \n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            NN_CONFIG['model_save_path'], \n",
    "            monitor='val_loss', \n",
    "            save_best_only=True,\n",
    "            verbose=0\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    print(\"Iniciando treinamento...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=NN_CONFIG['epochs'],\n",
    "        batch_size=NN_CONFIG['batch_size'],\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"Treinamento concluído.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b2ade0-eaab-4801-b29c-8f1fe1ddeaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_static_plot(fig, filename):\n",
    "    \"\"\"Salva imagem estática sem título e exibe interativa.\"\"\"\n",
    "    filepath = os.path.join(OUTPUT_IMG_DIR, filename)\n",
    "    \n",
    "    # Remove título para salvar versão limpa\n",
    "    layout_bkp = fig.layout.title.text\n",
    "    fig.update_layout(title=None)\n",
    "    fig.write_image(filepath, scale=2)\n",
    "    \n",
    "    # Restaura título e exibe\n",
    "    fig.update_layout(title=layout_bkp)\n",
    "    fig.show()\n",
    "\n",
    "if 'history' in locals():\n",
    "    hist_df = pd.DataFrame(history.history)\n",
    "    hist_df['epoch'] = history.epoch\n",
    "    \n",
    "    fig_hist = px.line(\n",
    "        hist_df, \n",
    "        x='epoch', \n",
    "        y=['loss', 'val_loss'], \n",
    "        labels={'value': 'Erro (MAE)', 'epoch': 'Época', 'variable': 'Métrica'},\n",
    "        title='Curva de Aprendizado: Treino vs Validação',\n",
    "        color_discrete_sequence=['#636EFA', '#EF553B']\n",
    "    )\n",
    "    fig_hist.update_layout(template='plotly_white')\n",
    "    save_static_plot(fig_hist, \"learning_curve.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411bbe42-8692-49c5-ac43-05bfcd7f90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'model' in locals():\n",
    "    print(\"Avaliando modelo no conjunto de teste...\")\n",
    "    \n",
    "    # Carregar melhor modelo salvo\n",
    "    best_model = tf.keras.models.load_model(NN_CONFIG['model_save_path'], compile=False)\n",
    "    best_model.compile(optimizer='adam', loss='mae', metrics=['mse'])\n",
    "    \n",
    "    # Predição e Inversão da Escala\n",
    "    y_pred_scaled = best_model.predict(X_test)\n",
    "    y_pred_real = output_scaler.inverse_transform(y_pred_scaled)\n",
    "    y_test_real = output_scaler.inverse_transform(y_test)\n",
    "    \n",
    "    # Montagem do DataFrame de Resultados\n",
    "    results_df = test_df[['dev_eui', 'timestamp']].copy().reset_index(drop=True)\n",
    "    results_df['gt_x'] = y_test_real[:, 0]\n",
    "    results_df['gt_y'] = y_test_real[:, 1]\n",
    "    results_df['pred_x'] = y_pred_real[:, 0]\n",
    "    results_df['pred_y'] = y_pred_real[:, 1]\n",
    "    \n",
    "    # Cálculo do Erro Euclidiano (Metros)\n",
    "    results_df['error_meters'] = np.sqrt(\n",
    "        (results_df['gt_x'] - results_df['pred_x'])**2 + \n",
    "        (results_df['gt_y'] - results_df['pred_y'])**2\n",
    "    )\n",
    "    \n",
    "    # Salvamento\n",
    "    results_df.to_csv('nn_validation_results.csv', index=False)\n",
    "    \n",
    "    # Métricas Finais\n",
    "    mae = results_df['error_meters'].mean()\n",
    "    rmse = np.sqrt((results_df['error_meters']**2).mean())\n",
    "    print(f\"\\n--- Resultados Finais (Teste) ---\\nMAE: {mae:.2f} m\\nRMSE: {rmse:.2f} m\")\n",
    "    \n",
    "    # Visualização da Distribuição de Erro\n",
    "    fig_dist = px.histogram(\n",
    "        results_df, \n",
    "        x='error_meters', \n",
    "        nbins=50,\n",
    "        title='Distribuição de Erro de Posicionamento (Conjunto de Teste)',\n",
    "        labels={'error_meters': 'Erro (metros)'},\n",
    "        opacity=0.7,\n",
    "        color_discrete_sequence=['green']\n",
    "    )\n",
    "    fig_dist.add_vline(x=mae, line_dash=\"dash\", annotation_text=f\"Média: {mae:.1f}m\")\n",
    "    fig_dist.update_layout(template='plotly_white', bargap=0.1)\n",
    "    \n",
    "    save_static_plot(fig_dist, \"error_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e8a17-6b6e-4226-83e1-584937f6b45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
